from google import genai
from google.genai import types
from openai import OpenAI
import os
from typing import Dict, Any, Optional

async def test_gemini_connection(api_key: str, model_name: str = "gemini-1.5-flash") -> Dict[str, Any]:
    """æµ‹è¯• Gemini è”é€šæ€§ (ä½¿ç”¨æœ€æ–° google-genai SDK)"""
    client = None
    try:
        api_key = api_key.strip()
        # æ—¥å¿—è®°å½•çš„å…³é”®ç‰‡æ®µä»¥ç¡®è®¤ Key æ˜¯å¦æ›´æ–°
        key_log = f"{api_key[:6]}...{api_key[-4:]}" if len(api_key) > 10 else "invalid"
        print(f"DEBUG: Testing Gemini with key {key_log} and model {model_name} (google-genai SDK)")
        
        client = genai.Client(api_key=api_key)
        
        # å®˜æ–¹å»ºè®®æ¨¡å‹åä¸å¸¦ models/
        clean_model_name = model_name.replace("models/", "")
        
        # å°è¯•ç”Ÿæˆæç®€å†…å®¹æµ‹è¯•é…é¢å’Œæœ‰æ•ˆæ€§
        response = client.models.generate_content(
            model=clean_model_name,
            contents="ping",
            config=types.GenerateContentConfig(
                max_output_tokens=10
            )
        )
        
        if response.text:
            return {"success": True, "message": f"Gemini ({clean_model_name}) è¿æ¥æˆåŠŸï¼"}
        return {"success": False, "message": "æ¨¡å‹è¿”å›ç©ºå“åº”"}
    except Exception as e:
        msg = str(e)
        if "429" in msg:
            return {
                "success": False, 
                "message": f"Gemini é…é¢ä¸Šé™ (429): æœåŠ¡å™¨è¿”å›é…é¢å·²æ»¡ã€‚è¿™é€šå¸¸æ˜¯å› ä¸ºè¯¥ Key æ‰€å±çš„é¡¹ç›®å·²è€—å°½å…è´¹é¢åº¦ï¼Œæˆ–è€…é¢‘ç¹è°ƒç”¨ã€‚è¯·å°è¯•æ£€æŸ¥å…¶å®ƒ Key æˆ–ç­‰å¾… 1 åˆ†é’Ÿå†è¯•ã€‚"
            }
        elif "404" in msg:
            try:
                # å°è¯•åˆ—å‡ºå¯ç”¨æ¨¡å‹ä»¥è°ƒè¯•
                print("DEBUG: 404 Error encountered. Listing available models...")
                paged_list = client.models.list(config={"page_size": 50})
                available_models = [m.name for m in paged_list]
                print(f"DEBUG: Available models: {available_models}")
                return {"success": False, "message": f"æ¨¡å‹æœªæ‰¾åˆ° (404): å½“å‰ Key å¯ç”¨æ¨¡å‹: {', '.join([m.split('/')[-1] for m in available_models[:5]])}... è¯·æŸ¥çœ‹åç«¯æ—¥å¿—è·å–å®Œæ•´åˆ—è¡¨ã€‚"}
            except Exception as list_err:
                print(f"DEBUG: Failed to list models: {list_err}")
                
            return {"success": False, "message": f"æ¨¡å‹æœªæ‰¾åˆ° (404): æœªæ‰¾åˆ°åä¸º {model_name} çš„æ¨¡å‹ã€‚è¯·å°è¯•ä½¿ç”¨å®Œæ•´åç§°å¦‚ gemini-1.5-flash-001"}
        elif "400" in msg:
            return {"success": False, "message": f"å‚æ•°é”™è¯¯ (400): è¯·æ£€æŸ¥æ¨¡å‹å {model_name} æ˜¯å¦æ­£ç¡®ã€‚å»ºè®®å°è¯•: gemini-1.5-flash"}
        return {"success": False, "message": f"Gemini è¿æ¥å¤±è´¥: {msg}"}

async def test_kimi_connection(api_key: str, model_name: str = "kimi-k2.5") -> Dict[str, Any]:
    """æµ‹è¯• Kimi (Moonshot) è”é€šæ€§"""
    try:
        api_key = api_key.strip()
        client = OpenAI(
            api_key=api_key,
            base_url="https://api.moonshot.cn/v1",
        )
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": "ping"}],
        )
        if response.choices[0].message.content:
            return {"success": True, "message": "Kimi è¿æ¥æˆåŠŸï¼"}
        return {"success": False, "message": "æ¨¡å‹è¿”å›ç©ºå“åº”"}
    except Exception as e:
        return {"success": False, "message": f"Kimi è¿æ¥å¤±è´¥: {str(e)}"}

async def generate_summary(
    api_key: str, 
    model_type: str, 
    model_name: str, 
    log_content: str
) -> Optional[str]:
    """ç”Ÿæˆå‘¨æŠ¥æ‘˜è¦"""
    prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é«˜çº§ç”Ÿäº§åŠ›é¡¾é—®ã€‚è¯·æ ¹æ®ä»¥ä¸‹æ—¥å¿—è®°å½•ç”Ÿæˆæœ¬å‘¨å‘¨æŠ¥æ€»ç»“ã€‚
è¦æ±‚ï¼š1. å¿…é¡»ä½¿ç”¨ä¸­æ–‡ã€‚ 2. ä¸¥æ ¼ JSON è¾“å‡ºã€‚ 3. æ‘˜è¦éœ€åŒ…å«å¯¹æˆå°±çš„è®¤å¯ã€‚

æ—¥å¿—å†…å®¹ï¼š
{log_content}

è¾“å‡º JSON æ ¼å¼å‚è€ƒï¼š
{{
  "executiveSummary": "æ€»ç»“å†…å®¹...",
  "focusAreas": [{{ "name": "é¢†åŸŸ", "percentage": 80 }}],
  "pulseStats": {{ "completed": 5, "completedChange": 1, "deepWorkHours": 10, "deepWorkAvg": 2 }},
  "highlights": [{{ "title": "äº®ç‚¹", "description": "æè¿°", "icon": "emoji", "category": "åˆ†ç±»", "timestamp": "æ—¶é—´" }}]
}}
"""
    try:
        api_key = api_key.strip()
        if model_type == "gemini":
            client = genai.Client(api_key=api_key)
            model_name = model_name.replace("models/", "")
            response = client.models.generate_content(
                model=model_name,
                contents=prompt
            )
            return response.text
        elif model_type == "kimi":
            client = OpenAI(
                api_key=api_key,
                base_url="https://api.moonshot.cn/v1",
            )
            response = client.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"}
            )
            return response.choices[0].message.content
    except Exception as e:
        print(f"Generation error: {e}")
        return None
    return None

async def aggregate_daily_logs(
    api_key: str,
    model_type: str,
    model_name: str,
    logs: list[str]
) -> Optional[str]:
    """å°†ç¢ç‰‡åŒ–çš„è®°å½•èšåˆä¸ºç»“æ„åŒ–çš„æ—¥æŠ¥"""
    if not logs:
        return None
        
    log_text = "\n".join([f"- {l}" for l in logs])
    prompt = f"""ä½ æ˜¯ä¸€ä½æè‡´é«˜æ•ˆçš„ç”Ÿäº§åŠ›æ•™ç»ƒã€‚ä»¥ä¸‹æ˜¯ç”¨æˆ·ä»Šå¤©çš„ç¢ç‰‡åŒ–è®°å½•å’Œæ„Ÿæ‚Ÿï¼š
{log_text}

è¯·å°†è¿™äº›è®°å½•æ•´ç†æˆä¸€ä»½æœ‰æ·±åº¦çš„â€œæ¯æ—¥æ´å¯Ÿ (Daily Insight)â€ã€‚
è¦æ±‚ï¼š
1. é£æ ¼ç®€æ´ã€ä¸“ä¸šä¸”å¯Œæœ‰å¯å‘æ€§ã€‚
2. ç»“æ„æ¸…æ™°ï¼ŒåŒ…å«ï¼š
   - ğŸ“Œ æ ¸å¿ƒäº‹é¡¹æ€»ç»“
   - ğŸ’¡ é—ªå¿µä¸æ„Ÿæ‚Ÿæç‚¼
   - ğŸ› ï¸ ä¸‹ä¸€æ­¥è¡ŒåŠ¨å»ºè®®
3. ä½¿ç”¨ä¸­æ–‡ã€‚
4. é•¿åº¦é€‚ä¸­ï¼Œé¿å…å†—ä½™ã€‚
"""
    try:
        api_key = api_key.strip()
        if model_type == "gemini":
            client = genai.Client(api_key=api_key)
            model_name = model_name.replace("models/", "")
            response = client.models.generate_content(
                model=model_name,
                contents=prompt
            )
            return response.text
        elif model_type == "kimi":
            client = OpenAI(
                api_key=api_key,
                base_url="https://api.moonshot.cn/v1",
            )
            response = client.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": prompt}]
            )
            return response.choices[0].message.content
    except Exception as e:
        print(f"Aggregation error: {e}")
        return None
    return None
